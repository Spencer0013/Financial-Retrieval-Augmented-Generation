## Financial Retrieval-Augmented Generation (RAG) â€” Finance Domain

A practical, end-to-end RAG pipeline for finance questions powered by **Pinecone hybrid search (dense + sparse)**, `sentence-transformers` embeddings, and a Streamlit app for interactive querying. It includes notebooks for data prep and evaluation plus a lightweight UI that returns an answer grounded in the top-ranked passage.

## Demo (Screenshot)
<img width="1918" height="1004" alt="Screenshot 2025-09-29 101201" src="https://github.com/user-attachments/assets/36f6736a-7372-4a75-a340-a2e039f3f264" />

---

## Highlights

- Dense (`sentence-transformers/all-MiniLM-L6-v2`) + Sparse (BM25 via `pinecone-text`) + Hybrid in Pinecone  
- Pinecone serverless index for storage and retrieval  
- Reproducible evaluation with provided notebook and CSV  
- Streamlit app for quick demos  
- RAG chain that constrains the model to only use retrieved context (and say â€œI donâ€™t knowâ€ if the answer is not supported)

---

## Project Structure

```text
â”œâ”€â”€ app.py                        # Streamlit application for interactive Q/A
â”œâ”€â”€ helpers.py                    # Retrieval + RAG chain builder, judge prompt
â”œâ”€â”€ evaluation.py                 # Batch evaluation script
â”œâ”€â”€ data_preparation.ipynb        # Data cleaning / chunking / indexing workflow
â”œâ”€â”€ hybrid_search.ipynb           # Retrieval experiments and diagnostics
â”œâ”€â”€ bm25_values.json              # Saved BM25 state (sparse encoder stats)
â”œâ”€â”€ eval_results.csv              # Example evaluation output
â”œâ”€â”€ requirements.txt              # Dependencies
â””â”€â”€ README.md
```

> **Note:** earlier versions referenced Qdrant. The current pipeline uses **Pinecone** as the vector DB and hybrid retriever.

---

## 1ï¸âƒ£ Start / Prepare Pinecone

You do not run Pinecone locally with Docker â€” itâ€™s a managed service.  
Instead:

1. Create a Pinecone account / API key.  
2. Set environment variables (see below).  
3. The code will create (or reuse) a serverless Pinecone index with the expected name (default: `fiqa-hybrid`).

**Index details (from `helpers.py`):**
- Dimension: `384`
- Metric: `dotproduct`
- Cloud/region: `aws` / `us-east-1` (can be changed in code)

---

## 2ï¸âƒ£ Create the Index & Upload Vectors

The workflow in `data_preparation.ipynb` / ingestion script does the following:

- Load finance corpus / notebook chunks  
- Fit a BM25 encoder:
  ```python
  from pinecone_text.sparse import BM25Encoder
  bm25 = BM25Encoder().default()
  bm25.fit(texts)
  bm25.dump("bm25_values.json")
  ```
- Compute dense embeddings with `sentence-transformers/all-MiniLM-L6-v2`
- Upsert **both** dense and sparse vectors into the Pinecone index along with metadata

> âš ï¸ After you upsert once, your vectors live in Pinecone permanently.  
> You do **not** need to re-upsert unless you add new data.  
> Keep the `bm25_values.json` file â€” itâ€™s required at query time to encode new user queries into sparse form.

---

## 3ï¸âƒ£ Run the App

Set your environment variables in a `.env` file or via shell.  
**Required:**

```bash
OPENAI_API_KEY="your OpenAI API key"
PINECONE_API_KEY="your Pinecone API key"
PINECONE_INDEX_NAME="fiqa-hybrid"
BM25_STATE_PATH="C:\path\to\bm25_values.json"
```

Then launch:

```bash
streamlit run app.py
```

### What `app.py` does
- Loads env vars  
- Loads the saved BM25 state (`bm25_values.json`)  
- Connects to the existing Pinecone index (`fiqa-hybrid`)  
- Builds a RAG chain with:
  - `PineconeHybridSearchRetriever` (dense + sparse hybrid search)
  - `gpt-4o-mini` as the generator  
- Lets you ask finance questions and returns grounded answers

---

## ğŸ§  Using the App

1. Type a question (e.g., â€œWhat is corporate finance?â€).  
2. Submit.  
3. The app:
   - Retrieves top-matching passages from Pinecone using hybrid search  
   - Builds context for the LLM  
   - Calls the generator model  
   - Displays the final grounded answer  

If the answer is not supported by the retrieved context, the model will respond: **â€œI donâ€™t know.â€**

---

## âš™ï¸ Configuration

### Pinecone connection
- `PINECONE_API_KEY` must be set  
- `PINECONE_INDEX_NAME` defaults to `fiqa-hybrid`  
- The code will create the index automatically if it doesnâ€™t exist (serverless spec)

### BM25 state
- `BM25_STATE_PATH` must point to `bm25_values.json`  
- This file is produced once during ingestion  
- Without it, retrieval fails with:  
  `RuntimeError: BM25 state not found ... (dump it first)`

### Generator model
- Uses `gpt-4o-mini` via `langchain-openai`  

### Embedding model
- Dense encoder: `sentence-transformers/all-MiniLM-L6-v2`

---

## ğŸ“Š Retrieval Evaluation

You can batch-score the system with:

```bash
python evaluation.py
```

This script:
- Loads a list of test queries (`fiqa/queries.jsonl`)
- Builds the same retriever + RAG chain as the app
- Generates answers and evaluates them using a judge model (`gpt-4o-mini`)
- Computes:
  - **correctness** â€” does the answer address the question accurately and completely?  
  - **groundedness** â€” is it supported only by retrieved context?

Results are saved to **`eval_results.csv`**.

### Example output columns
| Field | Description |
|--------|--------------|
| question | The query asked |
| answer | Model-generated response |
| correctness | Score (0â€“1) |
| groundedness | Score (0â€“1) |
| judge_notes | Short textual rationale |

---

## ğŸ“ˆ Metrics & Reporting

From the evaluation CSV you can compute:

- Overall relevance / correctness rate  
- Groundedness (hallucination control)  
- Breakdowns by retrieval configuration  
- Slices of low-performing examples for manual inspection  

Higher groundedness = stronger retrieval-grounded responses.

---

## ğŸ”„ How It Works (End-to-End)

1. **Chunking & Embedding**  
   Split finance data into passages â†’ create dense embeddings and BM25 sparse vectors.  

2. **Indexing**  
   Create Pinecone index â†’ upsert dense + sparse + metadata.  

3. **Querying**  
   Load BM25 + dense encoder â†’ perform hybrid retrieval via `PineconeHybridSearchRetriever`.  

4. **Answering**  
   Retrieved docs â†’ passed as `context` to `gpt-4o-mini` â†’ returns a concise, grounded answer.  

5. **Evaluation**  
   Use `evaluation.py` to assess correctness & groundedness with the same LLM as a judge.

---

## ğŸ™Œ Acknowledgements

- **Pinecone** for hybrid dense/sparse retrieval and vector storage  
- **pinecone-text** for BM25Encoder  
- **sentence-transformers** for MiniLM embeddings  
- **LangChain** for retrieval + RAG orchestration  
- **OpenAI gpt-4o-mini** for answering and judging  

---

## ğŸªª License

Add your preferred license (e.g., MIT) to this repository.
